# Thinking about Multi-Task Learning

Thinking about how multi-task learning(MTL) works in NLP. Inspired by [the work from Ruder](https://arxiv.org/pdf/1706.05098.pdf).

The repository aims for thinking about the advantages of MTL in NLP. Why it works and how to choose the helpful auxiliary tasks for the main task. It may be a hard work. However, BERT, a hit model, also give us a reason to explore the rules of language or inference, which defines a neat task for the language model. 
